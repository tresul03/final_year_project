{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to investigate normalisation's effect on the model's performance.\n",
    "\n",
    "Remember how I got a very differing set of MSEs for $y_{0}$ and $y_{1}$ predictions in multidimensionality? I'm going to see if I can resolve that here.\n",
    "\n",
    "See, when I computed the MSEs for my outputs, since *they* differed in scale, so did the scale of their MSEs. Normalisation should prevent this from happening, since they'll both share a common scale, meaning their MSEs should be more easily interpretable.\n",
    "\n",
    "I'm going to use the following formula to normalise both $x$ and $y$:\n",
    "\n",
    "$x_{n}^{'} = \\frac{x- \\langle x \\rangle}{\\sigma \\langle x \\rangle}$, $y_{n}^{'} = \\frac{y- \\langle y \\rangle}{\\sigma \\langle y \\rangle}$.\n",
    "\n",
    "I'm also going to use same the model architecture as multidimensionality's one.\n",
    "\n",
    "I'm changing how inputs are mapped a little, though:\n",
    "\n",
    "$x_{0}, x_{1} \\rightarrow y_{0}; \\; x_{0}, x_{1} \\rightarrow y_{1}.$ Let $x = (x_{0}, x_{1})$.\n",
    "\n",
    "$f_{0}(x) = y_{0} = 4x_{0}^{5} + 12x_{1}^{3} - 5$.\n",
    "\n",
    "$f_{1}(x) = y_{1} = 3x_{0}^{4} - 7x_{1}^{2} + 9x_{0}x_{1}$.\n",
    "\n",
    "With that, I'll:\n",
    "* Import my libraries\n",
    "* Declare my model's architecture\n",
    "* Normalise my dataset\n",
    "* Train my model\n",
    "* Test my model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchbnn as bnn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#allocating datasets and model to GPU for speed's sake\n",
    "is_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualOutputBNN(nn.Module):\n",
    "    def __init__(self, no_of_neurones, dropout_prob):\n",
    "        super(DualOutputBNN, self).__init__()\n",
    "        self.shared_layer = nn.Sequential( #this is the input layer\n",
    "            bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=2, out_features=no_of_neurones),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "        )\n",
    "\n",
    "        self.output_layer_y0 = nn.Sequential( #this is the output layer for y0\n",
    "            bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=no_of_neurones, out_features=no_of_neurones),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=no_of_neurones, out_features=1)\n",
    "        )\n",
    "        self.output_layer_y1 = nn.Sequential( #this is the output layer for y1\n",
    "            bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=no_of_neurones, out_features=no_of_neurones),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=no_of_neurones, out_features=1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x): #this is the forward pass, run automatically when you call the model\n",
    "        shared = self.shared_layer(x)\n",
    "        y0 = self.output_layer_y0(shared)\n",
    "        y1 = self.output_layer_y1(shared)\n",
    "        return y0, y1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_model(no_of_neurones: int, dropout_prob: float, lr: float = 0.01) -> tuple:\n",
    "    \"\"\"\n",
    "    Initialise the DualOutputBNN model with its loss functions and optimizer.\n",
    "\n",
    "    Parameters:\n",
    "    - no_of_neurones (int): Number of neurons in the hidden layer.\n",
    "    - dropout_prob (float): Dropout probability.\n",
    "    - lr (float): Learning rate for the optimizer. Default is 0.01.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing the initialized model, MSE loss function, KL loss function, KL weight, and optimizer.\n",
    "    \"\"\"\n",
    "\n",
    "    model = DualOutputBNN(no_of_neurones, dropout_prob).to(device)\n",
    "\n",
    "    mse_loss = nn.MSELoss().to(device)\n",
    "    kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False).to(device)\n",
    "    kl_weight = 0.01\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    return model, mse_loss, kl_loss, kl_weight, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_attributes, x_train, y_train, epochs: int):\n",
    "    \"\"\"\n",
    "    Train a Bayesian Neural Network model for a specified number of epochs.\n",
    "\n",
    "    Parameters:\n",
    "    - model_attributes: A tuple containing the model, loss functions, kl_weight, and optimizer.\n",
    "    - x_train (torch.Tensor): Input tensor for the training data.\n",
    "    - y_train (torch.Tensor): Target tensor for the training data.\n",
    "    - epochs (int): Number of epochs to train the model.\n",
    "\n",
    "    Returns:\n",
    "    - model (torch.nn.Sequential): The trained neural network model.\n",
    "\n",
    "    Ensures that training data and model are on the same device for efficiency.\n",
    "    \"\"\"\n",
    "\n",
    "    model, mse_loss, kl_loss, kl_weight, optimizer = model_attributes\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(epochs): \n",
    "        y0_pred, y1_pred = model(x_train)\n",
    "        y0_mse, y1_mse = mse_loss(y0_pred, torch.unsqueeze(torch.Tensor(y_train[:,0]), dim=1)), mse_loss(y1_pred, torch.unsqueeze(torch.Tensor(y_train[:,1]), dim=1))\n",
    "        kl = kl_loss(model)\n",
    "        y0_cost, y1_cost = y0_mse + kl_weight * kl, y1_mse + kl_weight * kl\n",
    "        cost = y0_cost + y1_cost\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # print(f\"- Cost: {cost.item():.3f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, x_test):\n",
    "    \"\"\"\n",
    "    Test a Bayesian Neural Network model to produce predictions along with mean and standard deviation.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Sequential): The trained neural network model.\n",
    "    - func (callable): The target function to compare against the model's predictions.\n",
    "\n",
    "    Returns:\n",
    "    - mean_model_results (numpy.ndarray): Array of mean predictions from the model.\n",
    "    - std_model_results (numpy.ndarray): Array of standard deviations of the predictions.\n",
    "    - x_test (torch.Tensor): Testing dataset inputs.\n",
    "    - y_test (torch.Tensor): Testing dataset targets.\n",
    "\n",
    "    The testing data is moved to the device specified by the global `device` variable.\n",
    "    \"\"\"\n",
    "\n",
    "    #producing predictions of model of testing data, as well as mean and standard deviation of predictions\n",
    "    model.eval().cpu()\n",
    "    y0_pred, y1_pred = model(x_test)[0], model(x_test)[1]\n",
    "\n",
    "    y0_pred, y1_pred = np.array([model(x_test)[0].detach().numpy() for _ in range(500)])[:,:,0].T, np.array([model(x_test)[1].detach().numpy() for _ in range(500)])[:,:,0].T\n",
    "\n",
    "    mean_y0_results, std_y0_results = np.array([np.mean(y0_pred[i]) for i in range(y0_pred.shape[0])]), np.array([np.std(y0_pred[i]) for i in range(y0_pred.shape[0])])\n",
    "    mean_y1_results, std_y1_results = np.array([np.mean(y1_pred[i]) for i in range(y1_pred.shape[0])]), np.array([np.std(y1_pred[i]) for i in range(y1_pred.shape[0])])\n",
    "\n",
    "    return mean_y0_results, std_y0_results, mean_y1_results, std_y1_results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(df: pd.DataFrame, func0, func1):\n",
    "    #splitting data into training and testing sets\n",
    "    x_train, x_test = train_test_split(df[[\"x0\", \"x1\"]], test_size=0.2, random_state=1)\n",
    "\n",
    "    #reset indices and drop old indices of train and test sets\n",
    "    x_train.reset_index(drop=True, inplace=True)\n",
    "    x_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #train_test_split causes data unordering. I fix that with the lines below\n",
    "    \n",
    "    #generating y values\n",
    "    y_train, y_test = pd.DataFrame(), pd.DataFrame()\n",
    "    y_train[\"y0\"], y_train[\"y1\"] = func0(torch.Tensor(x_train[\"x0\"]), torch.Tensor(x_train[\"x1\"])), func1(torch.Tensor(x_train[\"x0\"]), torch.Tensor(x_train[\"x1\"]))\n",
    "    y_test[\"y0\"], y_test[\"y1\"] = func0(torch.Tensor(x_test[\"x0\"]), torch.Tensor(x_test[\"x1\"])), func1(torch.Tensor(x_test[\"x0\"]), torch.Tensor(x_test[\"x1\"]))\n",
    "\n",
    "    #normalising dataset\n",
    "    normalise = lambda x: (x - np.mean(x)) / (np.std(x)*np.mean(x))\n",
    "\n",
    "    x_train[\"x0\"], x_train[\"x1\"] = normalise(x_train[\"x0\"]), normalise(x_train[\"x1\"])\n",
    "    x_test[\"x0\"], x_test[\"x1\"] = normalise(x_test[\"x0\"]), normalise(x_test[\"x1\"])\n",
    "    print(type(x_train), type(x_test))\n",
    "    \n",
    "    #sort all columns in ascending order, for x_train and x_test\n",
    "    x_train = x_train.apply(lambda x: x.sort_values().values)\n",
    "    x_test = x_test.apply(lambda x: x.sort_values().values)\n",
    "    \n",
    "\n",
    "    # print(x_train[\"x0\"], x_train[\"x1\"])\n",
    "\n",
    "    y_train[\"y0\"], y_train[\"y1\"] = normalise(y_train[\"y0\"]), normalise(y_train[\"y1\"])\n",
    "    y_test[\"y0\"], y_test[\"y1\"] = normalise(y_test[\"y0\"]), normalise(y_test[\"y1\"])\n",
    "    print(len(y_train), len(y_test))\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Dataset\n",
    "\n",
    "Of course, eventually, when I'm working with real data, this'll be more like \"*Processing* the Dataset\", but we're seeing what'll happen theoretically, so creating data is fine here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'>\n",
      "1600 400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-10.331235</td>\n",
       "      <td>-21.527390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-10.326907</td>\n",
       "      <td>-21.502542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-10.299686</td>\n",
       "      <td>-21.492569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-10.298235</td>\n",
       "      <td>-21.248283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-10.261984</td>\n",
       "      <td>-21.055599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>9.199833</td>\n",
       "      <td>19.439457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>9.248615</td>\n",
       "      <td>19.553877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>9.330134</td>\n",
       "      <td>19.596283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>9.347024</td>\n",
       "      <td>19.876175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>9.348701</td>\n",
       "      <td>20.287039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x0         x1\n",
       "0   -10.331235 -21.527390\n",
       "1   -10.326907 -21.502542\n",
       "2   -10.299686 -21.492569\n",
       "3   -10.298235 -21.248283\n",
       "4   -10.261984 -21.055599\n",
       "..         ...        ...\n",
       "395   9.199833  19.439457\n",
       "396   9.248615  19.553877\n",
       "397   9.330134  19.596283\n",
       "398   9.347024  19.876175\n",
       "399   9.348701  20.287039\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0, x1 = torch.rand(2000)*6 - 3, torch.rand(2000)*6 - 3\n",
    "\n",
    "#declaring target functions and their outputs\n",
    "clean_f0 = lambda x0, x1: 4*np.power(x0, 5) + 12*np.power(x1, 3) - 5\n",
    "clean_f1 = lambda x0, x1: 3*np.power(x0, 4) - 7*np.power(x1, 2) + 9*x0*x1\n",
    "clean_y0, clean_y1 = clean_f0(x0, x1), clean_f1(x0, x1)\n",
    "\n",
    "#declaring noisy functions and their outputs\n",
    "max_y0_scale, max_y1_scale = clean_y0.max()*0.2, clean_y1.max()*0.2 #scale for noisy function\n",
    "f0 = lambda x0, x1: clean_f0(x0, x1) + (max_y0_scale*torch.rand(x0.size()) - max_y0_scale/2)\n",
    "f1 = lambda x0, x1: clean_f1(x0, x1) + (max_y1_scale*torch.rand(x0.size()) - max_y1_scale/2)\n",
    "\n",
    "\n",
    "#processing dataset\n",
    "df = pd.DataFrame({'x0': x0, 'x1': x1})\n",
    "x_train, x_test, y_train, y_test = generate_dataset(df, f0, f1)\n",
    "\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting data to tensors for the model to use\n",
    "x_train, x_test, y_train, y_test = np.array(x_train), np.array(x_test), np.array(y_train), np.array(y_test)\n",
    "x_train, x_test, y_train, y_test = torch.Tensor(x_train).to(device), torch.Tensor(x_test), torch.Tensor(y_train).to(device), torch.Tensor(y_test)\n",
    "\n",
    "#where the machine learning occurs...\n",
    "model_attributes = initialise_model(1000, 0.3)\n",
    "model = train_model(model_attributes, x_train, y_train, 1000)\n",
    "mean_y0_results, std_y0_results, mean_y1_results, std_y1_results = test_model(model, x_test)\n",
    "\n",
    "mse_loss = nn.MSELoss().to(device)\n",
    "mean_y0_mse, mean_y1_mse = mse_loss(torch.Tensor(mean_y0_results), y_test[:,0]), mse_loss(torch.Tensor(mean_y1_results), y_test[:,1])\n",
    "\n",
    "print(f\"mean_y0_mse: {mean_y0_mse}, mean_y1_mse: {mean_y1_mse}\")\n",
    "print(f\"mean std(y0): {np.mean(std_y0_results)}, mean std(y1): {np.mean(std_y1_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, our MSEs are fine, but the issue of visualising this data remains.\n",
    "\n",
    "In this case, $x_{0}, x_{1}$ are 1D, so I can:\n",
    "* Plot $y_{0}$ against $x_{0}$, and then $x_{1}$\n",
    "* Plot $y_{1}$ against $x_{0}$, and then $x_{1}$\n",
    "\n",
    "Or I can just use PCA LOL."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
