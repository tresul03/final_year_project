{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to explore multi-dimensional inputs and outputs in this notebook.\n",
    "\n",
    "We're going to have $y = y(y_{0}, y_{1}, ..., y_{n})$, and $x = x(x_{0}, x_{1}, ..., x_{n})$. For simplicity's sake, let $n = 1$.\n",
    "\n",
    "I'm also going to assume each parameter of $x$ is mapped to its corresponding $y$ parameter: as in,\n",
    "\n",
    "$x_{0} \\rightarrow y_{0}$, and  $x_{1} \\rightarrow y_{1}$.\n",
    "\n",
    "There'll be functions mapping each input to its corresponding output, of course, for example,\n",
    "\n",
    "$y_{0} = x_{0}^{2}$, and $y_{1} = x_{1}^{4}$.\n",
    "\n",
    "I'm going to use more complex functions to strain the model a little more, but the real challenge is plotting these, because\n",
    "\n",
    "$x = [x_{0} \\; x_{1}]$, and $y = [y_{0} \\; y_{1}]$.\n",
    "\n",
    "A neural network usually has one output layer. I'm going to have to branch out the layer this time, such that each y-element has an output layer.\n",
    "\n",
    "Also, I'm going to use a dropout probability of 0.3, since, from the dropouts notebook, that seems to be the optimal choice.\n",
    "\n",
    "With that, I'll get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchbnn as bnn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allocating datasets and model to GPU for speed's sake\n",
    "is_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My data generation will be different here. I'm going to:\n",
    "* Have two linspaces: x0, x1\n",
    "* Have two  functions: fx0, fx1\n",
    "* Have two more linspaces: y0, y1\n",
    "* Form a dataframe (df) of the four linspaces\n",
    "* Use train_test_split on the df\n",
    "\n",
    "I'll think about device applications later.\n",
    "\n",
    "I'll also keep the functions simple:\n",
    "\n",
    "$y_{0} = f(x_{0}) = x^{5} -7x^{2} + 4$\n",
    "\n",
    "$y_{1} = f(x_{1}) = 3x^{2} + 2x - 5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y0</th>\n",
       "      <th>y1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.494648</td>\n",
       "      <td>0.494648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.109888</td>\n",
       "      <td>7.109888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.684902</td>\n",
       "      <td>6.684902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-38.015930</td>\n",
       "      <td>-38.015930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.194426</td>\n",
       "      <td>2.194426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          y0         y1\n",
       "0   0.494648   0.494648\n",
       "1   7.109888   7.109888\n",
       "2   6.684902   6.684902\n",
       "3 -38.015930 -38.015930\n",
       "4   2.194426   2.194426"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating dataset\n",
    "x0, x1 = torch.linspace(-2, 2, 1000), torch.linspace(-2, 2, 1000)\n",
    "\n",
    "fx0 = lambda x: np.power(x, 5) - 7*np.power(x, 2) + (torch.rand(x.size())*4 + 2)\n",
    "fx1 = lambda x: 3*np.power(x, 2) + 2*x - (torch.rand(x.shape)*5 + 2)\n",
    "\n",
    "y0 = fx0(x0)\n",
    "y1 = fx1(x1)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"x0\":x0,\n",
    "        \"x1\":x1,\n",
    "        \"y0\":y0,\n",
    "        \"y1\":y0\n",
    "    }\n",
    ")\n",
    "\n",
    "#df.head()\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[[\"x0\", \"x1\"]], df[[\"y0\", \"y1\"]], test_size=0.2, random_state=1)\n",
    "\n",
    "#reset indices and drop old indices of train and test sets\n",
    "x_train.reset_index(drop=True, inplace=True)\n",
    "x_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I'm working with multi-dimensional inputs and outputs, the model architecture has to change to accommodate the new dataset's properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualOutputBNN(nn.Module):\n",
    "    def __init__(self, no_of_neurones, dropout_prob):\n",
    "        super(DualOutputBNN, self).__init__()\n",
    "        self.shared_layer = nn.Sequential(\n",
    "            bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=2, out_features=no_of_neurones),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=no_of_neurones, out_features=no_of_neurones),\n",
    "        )\n",
    "        self.output_layer_y0 = bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=no_of_neurones, out_features=1)\n",
    "        self.output_layer_y1 = bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=no_of_neurones, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared = self.shared_layer(x)\n",
    "        y0 = self.output_layer_y0(shared)\n",
    "        y1 = self.output_layer_y1(shared)\n",
    "        return y0, y1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll now write a function to initialise the model above, in the same way that I did in previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_model(no_of_neurones: int, dropout_prob: float, lr: float = 0.01) -> tuple:\n",
    "    \"\"\"\n",
    "    Initialise the DualOutputBNN model with its loss functions and optimizer.\n",
    "\n",
    "    Parameters:\n",
    "    - no_of_neurones (int): Number of neurons in the hidden layer.\n",
    "    - dropout_prob (float): Dropout probability.\n",
    "    - lr (float): Learning rate for the optimizer. Default is 0.01.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing the initialized model, MSE loss function, KL loss function, KL weight, and optimizer.\n",
    "    \"\"\"\n",
    "\n",
    "    model = DualOutputBNN(no_of_neurones, dropout_prob).to(device)\n",
    "\n",
    "    mse_loss = nn.MSELoss().to(device)\n",
    "    kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False).to(device)\n",
    "    kl_weight = 0.01\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    return model, mse_loss, kl_loss, kl_weight, optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, I've got my dataset, split it into training and testing sets, declared my model architecture, and written the function to intialise it.\n",
    "\n",
    "What's next is to:\n",
    "* Write a function to train the model\n",
    "* Write a function to test the model\n",
    "* Find a way to plot the model's predictions based on testing data\n",
    "\n",
    "At some point, I should really make a class containing all of these functions so that I don't repeat them in every notebook..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_attributes, x_train, y_train, epochs: int):\n",
    "    \"\"\"\n",
    "    Train a Bayesian Neural Network model for a specified number of epochs.\n",
    "\n",
    "    Parameters:\n",
    "    - model_attributes: A tuple containing the model, loss functions, kl_weight, and optimizer.\n",
    "    - x_train (torch.Tensor): Input tensor for the training data.\n",
    "    - y_train (torch.Tensor): Target tensor for the training data.\n",
    "    - epochs (int): Number of epochs to train the model.\n",
    "\n",
    "    Returns:\n",
    "    - model (torch.nn.Sequential): The trained neural network model.\n",
    "\n",
    "    Ensures that training data and model are on the same device for efficiency.\n",
    "    \"\"\"\n",
    "\n",
    "    # assert x_train.device == y_train.device == device, \"Keep everything on the GPU for speed\"\n",
    "\n",
    "    model, mse_loss, kl_loss, kl_weight, optimizer = model_attributes\n",
    "    model.train()\n",
    "\n",
    "    # x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "    print(x_train.shape, y_train.shape)\n",
    "    # x_train, y_train = torch.Tensor(x_train).to(device), torch.Tensor(y_train).to(device)\n",
    "\n",
    "    print(f\"training model...\")\n",
    "    for _ in range(epochs):\n",
    "        y0_pred, y1_pred = model(x_train)\n",
    "        y0_mse, y1_mse = mse_loss(y0_pred, y_train[:,0]), mse_loss(y1_pred, y_train[:,1])\n",
    "        kl = kl_loss(model)\n",
    "        y0_cost, y1_cost = y0_mse + kl_weight * kl, y1_mse + kl_weight * kl\n",
    "        cost = y0_cost + y1_cost\n",
    "        # cost = mse + kl_weight * kl\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"- Cost: {cost.item():.3f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I already produced the test set in the beginning, producing testing data in test_model() not only becomes trivial, but, with the way I produce it in the function, error-prone. I can also remove the func parameter for this reason.\n",
    "\n",
    "The real problem is processing the produced data.\n",
    "\n",
    "I need to:\n",
    "* Predict both $y_{0}$ and $y_{1}$\n",
    "* Find each output's mean_model_results and std_model_results\n",
    "* Return a df: x_test, y_test, mean_model_results (for $y_{0},y_{1}$), std_model_results (for $y_{0}, y_{1}$)\n",
    "\n",
    "So basically I'm producing a results dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Test a Bayesian Neural Network model to produce predictions along with mean and standard deviation.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Sequential): The trained neural network model.\n",
    "    - func (callable): The target function to compare against the model's predictions.\n",
    "\n",
    "    Returns:\n",
    "    - mean_model_results (numpy.ndarray): Array of mean predictions from the model.\n",
    "    - std_model_results (numpy.ndarray): Array of standard deviations of the predictions.\n",
    "    - x_test (torch.Tensor): Testing dataset inputs.\n",
    "    - y_test (torch.Tensor): Testing dataset targets.\n",
    "\n",
    "    The testing data is moved to the device specified by the global `device` variable.\n",
    "    \"\"\"\n",
    "\n",
    "    #producing predictions of model of testing data, as well as mean and standard deviation of predictions\n",
    "    model.eval().cpu()\n",
    "    y0_pred, y1_pred = np.array([model(x_test)[0].detach().numpy() for _ in range(10000)])[:,:,0].T, np.array([model(x_test)[1].detach().numpy() for _ in range(10000)])[:,:,0].T\n",
    "    print(f\"produced predictions\")\n",
    "\n",
    "    mean_y0_results, std_y0_results = np.array([np.mean(y0_pred[i]) for i in range(y0_pred.shape[0])]), np.array([np.std(y0_pred[i]) for i in range(y0_pred.shape[0])])\n",
    "    mean_y1_results, std_y1_results = np.array([np.mean(y1_pred[i]) for i in range(y1_pred.shape[0])]), np.array([np.std(y1_pred[i]) for i in range(y1_pred.shape[0])])\n",
    "\n",
    "    print(f\"produced means and standard deviations\")\n",
    "    # model_results = np.array([model(x_test) for _ in range(10000)])[:,:,0].T\n",
    "    # mean_model_results = np.array([np.mean(model_results[i]) for i in range(model_results.shape[0])])\n",
    "    # std_model_results = np.array([np.std(model_results[i]) for i in range(model_results.shape[0])])\n",
    "\n",
    "    return mean_y0_results, std_y0_results, mean_y1_results, std_y1_results, x_test.cpu(), y_test.cpu() #!this function needs testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "torch.Size([800, 2]) torch.Size([800, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\resul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([800])) that is different to the input size (torch.Size([800, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Cost: 330.249\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\resul\\OneDrive\\Physics\\Final Year\\PH30036\\project_script\\src\\week5\\multidimensionality.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/resul/OneDrive/Physics/Final%20Year/PH30036/project_script/src/week5/multidimensionality.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m model_attributes \u001b[39m=\u001b[39m initialise_model(\u001b[39m1000\u001b[39m, \u001b[39m0.3\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/resul/OneDrive/Physics/Final%20Year/PH30036/project_script/src/week5/multidimensionality.ipynb#X21sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m model \u001b[39m=\u001b[39m train_model(model_attributes, x_train, y_train, \u001b[39m1000\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/resul/OneDrive/Physics/Final%20Year/PH30036/project_script/src/week5/multidimensionality.ipynb#X21sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m mean_y0_results, std_y0_results, mean_y1_results, std_y1_results, x_test, y_test \u001b[39m=\u001b[39m test_model(model, x_test, y_test)\n",
      "\u001b[1;32mc:\\Users\\resul\\OneDrive\\Physics\\Final Year\\PH30036\\project_script\\src\\week5\\multidimensionality.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/resul/OneDrive/Physics/Final%20Year/PH30036/project_script/src/week5/multidimensionality.ipynb#X21sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m#producing predictions of model of testing data, as well as mean and standard deviation of predictions\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/resul/OneDrive/Physics/Final%20Year/PH30036/project_script/src/week5/multidimensionality.ipynb#X21sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m model\u001b[39m.\u001b[39meval()\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/resul/OneDrive/Physics/Final%20Year/PH30036/project_script/src/week5/multidimensionality.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m y0_pred, y1_pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([model(x_test)[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mnumpy() \u001b[39mfor\u001b[39;49;00m _ \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(\u001b[39m10000\u001b[39;49m)])[:,:,\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mT, np\u001b[39m.\u001b[39marray([model(x_test)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10000\u001b[39m)])[:,:,\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mT\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/resul/OneDrive/Physics/Final%20Year/PH30036/project_script/src/week5/multidimensionality.ipynb#X21sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mproduced predictions\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/resul/OneDrive/Physics/Final%20Year/PH30036/project_script/src/week5/multidimensionality.ipynb#X21sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m mean_y0_results, std_y0_results \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([np\u001b[39m.\u001b[39mmean(y0_pred[i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(y0_pred\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])]), np\u001b[39m.\u001b[39marray([np\u001b[39m.\u001b[39mstd(y0_pred[i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(y0_pred\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])])\n",
      "\u001b[1;32mc:\\Users\\resul\\OneDrive\\Physics\\Final Year\\PH30036\\project_script\\src\\week5\\multidimensionality.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/resul/OneDrive/Physics/Final%20Year/PH30036/project_script/src/week5/multidimensionality.ipynb#X21sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m#producing predictions of model of testing data, as well as mean and standard deviation of predictions\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/resul/OneDrive/Physics/Final%20Year/PH30036/project_script/src/week5/multidimensionality.ipynb#X21sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m model\u001b[39m.\u001b[39meval()\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/resul/OneDrive/Physics/Final%20Year/PH30036/project_script/src/week5/multidimensionality.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m y0_pred, y1_pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([model(x_test)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10000\u001b[39m)])[:,:,\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mT, np\u001b[39m.\u001b[39marray([model(x_test)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10000\u001b[39m)])[:,:,\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mT\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/resul/OneDrive/Physics/Final%20Year/PH30036/project_script/src/week5/multidimensionality.ipynb#X21sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mproduced predictions\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/resul/OneDrive/Physics/Final%20Year/PH30036/project_script/src/week5/multidimensionality.ipynb#X21sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m mean_y0_results, std_y0_results \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([np\u001b[39m.\u001b[39mmean(y0_pred[i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(y0_pred\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])]), np\u001b[39m.\u001b[39marray([np\u001b[39m.\u001b[39mstd(y0_pred[i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(y0_pred\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])])\n",
      "File \u001b[1;32mc:\\Users\\resul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\resul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\resul\\OneDrive\\Physics\\Final Year\\PH30036\\project_script\\src\\week5\\multidimensionality.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/resul/OneDrive/Physics/Final%20Year/PH30036/project_script/src/week5/multidimensionality.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/resul/OneDrive/Physics/Final%20Year/PH30036/project_script/src/week5/multidimensionality.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     shared \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshared_layer(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/resul/OneDrive/Physics/Final%20Year/PH30036/project_script/src/week5/multidimensionality.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     y0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_layer_y0(shared)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/resul/OneDrive/Physics/Final%20Year/PH30036/project_script/src/week5/multidimensionality.ipynb#X21sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     y1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_layer_y1(shared)\n",
      "File \u001b[1;32mc:\\Users\\resul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\resul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\resul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\resul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\resul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\resul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchbnn\\modules\\linear.py:86\u001b[0m, in \u001b[0;36mBayesLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[39mOverriden.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_eps \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m :\n\u001b[1;32m---> 86\u001b[0m     weight \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_mu \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mexp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_log_sigma) \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39;49mrandn_like(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight_log_sigma)\n\u001b[0;32m     87\u001b[0m \u001b[39melse\u001b[39;00m :\n\u001b[0;32m     88\u001b[0m     weight \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_mu \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mexp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_log_sigma) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_eps\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = np.array(x_train), np.array(x_test), np.array(y_train), np.array(y_test)\n",
    "x_train, x_test, y_train, y_test = torch.Tensor(x_train).to(device), torch.Tensor(x_test), torch.Tensor(y_train).to(device), torch.Tensor(y_test)\n",
    "\n",
    "# x_train, x_test, y_train, y_test = torch.unsqueeze(torch.Tensor(x_train).to(device), dim=2), torch.unsqueeze(torch.Tensor(x_test), dim=2), torch.unsqueeze(torch.Tensor(y_train).to(device), dim=2), torch.unsqueeze(torch.Tensor(y_test), dim=2)\n",
    "\n",
    "# print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "# model_attributes = initialise_model(1000, 0.3)\n",
    "# model = train_model(model_attributes, x_train, y_train, 1000)\n",
    "# # model.eval()\n",
    "# mean_model_results, std_model_results, x_test, y_test = test_model(model, x_test, y_test)\n",
    "\n",
    "print(type(x_train), type(y_train), type(x_test), type(y_test))\n",
    "\n",
    "model_attributes = initialise_model(1000, 0.3)\n",
    "model = train_model(model_attributes, x_train, y_train, 1000)\n",
    "\n",
    "mean_y0_results, std_y0_results, mean_y1_results, std_y1_results, x_test, y_test = test_model(model, x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
