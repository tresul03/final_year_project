{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using 100 SKIRT Runs Instead of 6\n",
    "\n",
    "We have data for 100 SKIRT runs: more than 15 times the data we had initially.\n",
    "\n",
    "The aim here is to observe the effect of having more data on the model's performance.\n",
    "\n",
    "This time, I'm going to make sure that all inclinations for a given run are either in the test or train set, and not both, since they correspond to measurements of the same galaxy. We want the model to predict values for a galaxy at every inclination we want; having parts of a run in both sets, therefore, defeats our purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchbnn as bnn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "#allocating datasets and model to GPU for speed's sake\n",
    "is_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripleOutputBNN(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a class for a Bayesian Neural Network with 3 outputs.\n",
    "    Attributes:\n",
    "        no_of_neurones (int): number of neurones in the hidden layer.\n",
    "        dropout_prob (float): dropout probability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, no_of_neurones, dropout_prob):\n",
    "        \"\"\"\n",
    "        The constructor for TripleOutputBNN class.\n",
    "        \n",
    "        Parameters:\n",
    "            no_of_neurones (int): number of neurones in the hidden layer.\n",
    "            dropout_prob (float): dropout probability.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(TripleOutputBNN, self).__init__()\n",
    "        self.shared_layer = nn.Sequential( #this is the input layer\n",
    "            bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=3, out_features=no_of_neurones),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "        )\n",
    "\n",
    "        self.output_layer_y0 = nn.Sequential( #this is the output layer for y0\n",
    "            bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=no_of_neurones, out_features=no_of_neurones),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=no_of_neurones, out_features=113)\n",
    "        )\n",
    "        self.output_layer_y1 = nn.Sequential( #this is the output layer for y1\n",
    "            bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=no_of_neurones, out_features=no_of_neurones),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=no_of_neurones, out_features=113)\n",
    "        )\n",
    "        self.output_layer_y2 = nn.Sequential( #this is the output layer for y2\n",
    "            bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=no_of_neurones, out_features=no_of_neurones),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=no_of_neurones, out_features=113)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x): #this is the forward pass, run automatically when you call the model\n",
    "        \"\"\"\n",
    "        This function performs the forward pass.\n",
    "        Parameters:\n",
    "            x (tensor): input tensor.\n",
    "        \n",
    "        Returns:\n",
    "            y0 (tensor): output tensor for y0.\n",
    "            y1 (tensor): output tensor for y1.\n",
    "            y2 (tensor): output tensor for y2.\n",
    "        \"\"\"\n",
    "\n",
    "        shared = self.shared_layer(x)\n",
    "        y0 = self.output_layer_y0(shared)\n",
    "        y1 = self.output_layer_y1(shared)\n",
    "        y2 = self.output_layer_y2(shared)\n",
    "        return y0, y1, y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_model(no_of_neurones: int, dropout_prob: float, lr: float = 0.01) -> tuple:\n",
    "    \"\"\"\n",
    "    Initialise the DualOutputBNN model with its loss functions and optimizer.\n",
    "\n",
    "    Parameters:\n",
    "    - no_of_neurones (int): Number of neurons in the hidden layer.\n",
    "    - dropout_prob (float): Dropout probability.\n",
    "    - lr (float): Learning rate for the optimizer. Default is 0.01.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing the initialized model, MSE loss function, KL loss function, KL weight, and optimizer.\n",
    "    \"\"\"\n",
    "\n",
    "    model = TripleOutputBNN(no_of_neurones, dropout_prob).to(device)\n",
    "\n",
    "    mse_loss = nn.MSELoss().to(device)\n",
    "    kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False).to(device)\n",
    "    kl_weight = 0.01\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    return model, mse_loss, kl_loss, kl_weight, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_attributes, input_train, output_train, epochs: int, batch_size: int):\n",
    "    \"\"\"\n",
    "    Train the model using batch training.\n",
    "\n",
    "    Parameters:\n",
    "    - model_attributes (tuple): Tuple containing the initialized model, MSE loss function, KL loss function, KL weight, and optimizer.\n",
    "    - input_train (tensor): Input tensor for training.\n",
    "    - output_train (tensor): Output tensor for training.\n",
    "    - epochs (int): Number of epochs to train the model.\n",
    "    - batch_size (int): Batch size for training.\n",
    "\n",
    "    Returns:\n",
    "    - The trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    model, mse_loss, kl_loss, kl_weight, optimizer = model_attributes\n",
    "    model = model.train()\n",
    "\n",
    "    # Create a TensorDataset from input and output tensors\n",
    "    tensor_dataset = TensorDataset(input_train, output_train)\n",
    "\n",
    "    # Create a DataLoader for batch training\n",
    "    data_loader = DataLoader(tensor_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch_data, batch_labels in data_loader:\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            n_pred, f_pred, r_pred = model(batch_data)\n",
    "\n",
    "            # Calculate MSE loss for each output and KL divergence\n",
    "            n_mse = mse_loss(n_pred, batch_labels[:, 0])\n",
    "            f_mse = mse_loss(f_pred, batch_labels[:, 1])\n",
    "            r_mse = mse_loss(r_pred, batch_labels[:, 2])\n",
    "            kl = kl_loss(model)\n",
    "\n",
    "            # Calculate cost (MSE + KL)\n",
    "            n_cost, f_cost, r_cost = n_mse + kl_weight * kl, f_mse + kl_weight * kl, r_mse + kl_weight * kl\n",
    "            cost = n_cost + f_cost + r_cost\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            cost.backward()\n",
    "            # n_cost.backward(retain_graph=True)\n",
    "            # f_cost.backward(retain_graph=True)\n",
    "            # r_cost.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Optionally, print the cost at the end of each epoch or save checkpoints\n",
    "        print(f\"- Epoch {epoch + 1}: n cost: {n_cost.item():.3f}, f cost: {f_cost.item():.3f}, r cost: {r_cost.item():.3f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, model_attributes, input_test, output_test):\n",
    "    \"\"\"\n",
    "    Test the model.\n",
    "\n",
    "    Parameters:\n",
    "    - model (nn.Module): Trained model.\n",
    "    - model_attributes (tuple): Tuple containing the initialized model, MSE loss function, KL loss function, KL weight, and optimizer.\n",
    "    - input_test (tensor): Input tensor for testing.\n",
    "    - output_test (tensor): Output tensor for testing.\n",
    "\n",
    "    Returns:\n",
    "    - mean_n_results (tensor): Mean of the predictions for y0.\n",
    "    - std_n_results (tensor): Standard deviation of the predictions for y0.\n",
    "    - mean_f_results (tensor): Mean of the predictions for y1.\n",
    "    - std_f_results (tensor): Standard deviation of the predictions for y1.\n",
    "    - mean_r_results (tensor): Mean of the predictions for y2.\n",
    "    - std_r_results (tensor): Standard deviation of the predictions for y2.\n",
    "    \"\"\"\n",
    "\n",
    "    mse_loss, kl_loss, kl_weight, optimizer = model_attributes[1:]\n",
    "\n",
    "    #producing predictions of model of testing data, as well as mean and standard deviation of predictions\n",
    "    model = model.eval().cpu()\n",
    "    input_test = input_test.cpu()\n",
    "    output_test = output_test.cpu()\n",
    "\n",
    "    n_pred = np.array([model(input_test)[0].detach().numpy() for _ in range(500)]).T\n",
    "    f_pred = np.array([model(input_test)[1].detach().numpy() for _ in range(500)]).T\n",
    "    r_pred = np.array([model(input_test)[2].detach().numpy() for _ in range(500)]).T\n",
    "\n",
    "    print(n_pred.shape)\n",
    "\n",
    "    #for each prediction of row, find mean and standard deviation\n",
    "    mean_n_results = np.mean(n_pred, axis = 2)\n",
    "    std_n_results = np.std(n_pred, axis = 2)\n",
    "    mean_f_results = np.mean(f_pred, axis = 2)\n",
    "    std_f_results = np.std(f_pred, axis = 2)\n",
    "    mean_r_results = np.mean(r_pred, axis = 2)\n",
    "    std_r_results = np.std(r_pred, axis = 2)\n",
    "\n",
    "    print(mean_n_results.shape)\n",
    "\n",
    "    #find the cost of the model\n",
    "    n_mse = mse_loss(torch.unsqueeze(torch.Tensor(mean_n_results), dim = 2), torch.unsqueeze(torch.Tensor(output_test[:,0].T), dim=2))\n",
    "    f_mse = mse_loss(torch.unsqueeze(torch.Tensor(mean_f_results), dim = 2), torch.unsqueeze(torch.Tensor(output_test[:,1].T), dim=2))\n",
    "    r_mse = mse_loss(torch.unsqueeze(torch.Tensor(mean_r_results), dim = 2), torch.unsqueeze(torch.Tensor(output_test[:,2].T), dim=2))\n",
    "\n",
    "    kl = kl_loss(model)\n",
    "    n_cost, f_cost, r_cost = n_mse + kl_weight * kl, f_mse + kl_weight * kl, r_mse + kl_weight * kl\n",
    "    # cost = n_cost + f_cost + r_cost\n",
    "\n",
    "    print(f\"- n cost: {n_cost.item():.3f}, f cost: {f_cost.item():.3f}, r cost: {r_cost.item():.3f}\")\n",
    "\n",
    "    return mean_n_results, std_n_results, mean_f_results, std_f_results, mean_r_results, std_r_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Input Parameters from Input File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_params(filename: str, filepath: str = '../../data/radiative_transfer/input/'):\n",
    "    \"\"\"\n",
    "    Read the parameters from the input file.\n",
    "\n",
    "    Parameters:\n",
    "    - filename (str): Name of the input file.\n",
    "    - filepath (str): Path to the input file. Default is '../../data/radiative_transfer/input/'.\n",
    "\n",
    "    Returns:\n",
    "    - table (dict): Dictionary containing the parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    lines = open(filepath+filename, 'r').readlines()\n",
    "\n",
    "    keys = []\n",
    "    values = []\n",
    "    for i in range(len(lines)):\n",
    "\n",
    "        line_i = lines[i]\n",
    "        line1 = line_i.split('\\n')[0]\n",
    "        line2 = line1.split('#')[0]\n",
    "        line3 = line2.split('=')\n",
    "        line4 = []\n",
    "        for j in range(len(line3)):\n",
    "            line4.append( line3[j].strip(' ') )\n",
    "\n",
    "        if len(line4) == 2:\n",
    "            keys.append(line4[0])\n",
    "            line5 = line4[1].split(', ')\n",
    "            line5 = np.array(line5).astype(float)\n",
    "            if len(line5) == 1 and line4[0]!='theta':\n",
    "                line5 = line5[0]\n",
    "            values.append(line5)\n",
    "\n",
    "    table = dict(zip(keys, values) )\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Input Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parameter_files(filenames: list, filepath: str = \"../../../data/radiative_transfer/input/\"):\n",
    "    \"\"\"\n",
    "    Read the parameter files.\n",
    "\n",
    "    Parameters:\n",
    "    - filenames (list): List of filenames.\n",
    "    - filepath (str): Path to the parameter files. Default is '../../data/radiative_transfer/input/'.\n",
    "\n",
    "    Returns:\n",
    "    - list_log_mstar (np.array): Array of log of stellar mass.\n",
    "    - list_log_mdust_over_mstar (np.array): Array of log of dust mass over stellar mass.\n",
    "    - list_theta (np.array): Array of viewing angles.\n",
    "    \"\"\"\n",
    "\n",
    "    list_log_mstar = np.array([])\n",
    "    list_log_mdust = np.array([])\n",
    "    list_theta = np.array([])\n",
    "\n",
    "    for filename in filenames:\n",
    "        table = read_params(filename, filepath)\n",
    "        list_log_mstar = np.append(list_log_mstar, table['logMstar'])\n",
    "        list_log_mdust = np.append(list_log_mdust, table['logMdust'])\n",
    "        list_theta = np.append(list_theta, table['theta'])\n",
    "\n",
    "    list_log_mdust_over_mstar = list_log_mdust - list_log_mstar\n",
    "\n",
    "    return list_log_mstar, list_log_mdust_over_mstar, list_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_h5_file(filename: str, data, thetas, log_mstar, log_mdust_over_mstar, filepath: str = '../../../data/radiative_transfer/output/'):\n",
    "    \"\"\"\n",
    "    Read the output file.\n",
    "\n",
    "    Parameters:\n",
    "    - filename (str): Name of the output file.\n",
    "    - data (pd.DataFrame): DataFrame containing the data.\n",
    "    - thetas (list): List of viewing angles.\n",
    "    - log_mstar (float): log of stellar mass.\n",
    "    - log_mdust_over_mstar (float): log of dust mass over stellar mass.\n",
    "    - filepath (str): Path to the output file. Default is '../../data/radiative_transfer/output/'.\n",
    "\n",
    "    Returns:\n",
    "    - wvl (np.array): Rest-frame wavelength [micron].\n",
    "    - data (pd.DataFrame): DataFrame containing the data.\n",
    "    \"\"\"\n",
    "\n",
    "    normalise = lambda x: (x - np.mean(x)) / np.std(x)\n",
    "\n",
    "    filepath += filename \n",
    "    # print(filepath)\n",
    "\n",
    "    # Finding hdf keys\n",
    "    hdf_keys = np.array([])\n",
    "    with pd.HDFStore(filepath, 'r') as hdf:\n",
    "        hdf_keys = np.append(hdf_keys, hdf.keys())\n",
    "\n",
    "    for i in range(len(hdf_keys)):\n",
    "\n",
    "        table = pd.read_hdf(filepath, hdf_keys[i]) # Face-on view\n",
    "        wvl = table['wvl'].to_numpy(dtype=np.float64) # rest-frame wavelength [micron]\n",
    "        flux = table['flux'].to_numpy(dtype=np.float64) # flux [W/m^2]\n",
    "        r = table['r'].to_numpy(dtype=np.float64) # half-light radius [kpc]\n",
    "        n = table['n'].to_numpy(dtype=np.float64) # Sersic index\n",
    "\n",
    "        flux, r, n = normalise(flux), normalise(r), normalise(n)\n",
    "\n",
    "        data = pd.concat([data, pd.DataFrame({\"log_mstar\": log_mstar, \"log_mdust_over_mstar\": log_mdust_over_mstar, \"theta\": thetas[i], \"n\":[n], \"flux\":[flux], \"r\":[r]})], ignore_index=True)\n",
    "\n",
    "    return wvl, data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(data, params, files):\n",
    "    \"\"\"\n",
    "    Generate the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): DataFrame containing the data.\n",
    "    - params (list): List of parameter files.\n",
    "    - files (list): List of output files.\n",
    "\n",
    "    Returns:\n",
    "    - wavelength (np.array): Rest-frame wavelength [micron].\n",
    "    - data (pd.DataFrame): DataFrame containing the data.\n",
    "    \"\"\"\n",
    "\n",
    "    list_log_mstar, list_log_mdust_over_mstar, list_theta = read_parameter_files(params)\n",
    "\n",
    "    for i in range(len(files)):\n",
    "        wavelength, data = read_h5_file(files[i], data, np.sin(list_theta), list_log_mstar[i], list_log_mdust_over_mstar[i])\n",
    "\n",
    "    return wavelength, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Train & Test Sets to I/O Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensor(data):\n",
    "    \"\"\"\n",
    "    Convert the DataFrame to a tensor.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): DataFrame containing the data.\n",
    "\n",
    "    Returns:\n",
    "    - data (tensor): Tensor containing the data.\n",
    "    \"\"\"\n",
    "\n",
    "    data = data.applymap(np.array)\n",
    "    stacked_input_arrays = np.stack(data.apply(lambda row: np.stack(row, axis=0), axis=1).to_numpy())\n",
    "    data = torch.Tensor(stacked_input_arrays).to(device)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(parameter_files, h5_files):\n",
    "    \"\"\"\n",
    "    Split the dataset into training and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "    - parameter_files (list): List of parameter files.\n",
    "    - h5_files (list): List of output files.\n",
    "\n",
    "    Returns:\n",
    "    - input_train (list): List of input tensors for training.\n",
    "    - input_test (list): List of input tensors for testing.\n",
    "    - output_train (list): List of output tensors for training.\n",
    "    - output_test (list): List of output tensors for testing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    df = pd.DataFrame({\"parameter_files\": parameter_files, \"h5_files\": h5_files})\n",
    "    train, test = train_test_split(df, test_size=0.2, random_state=0)\n",
    "\n",
    "    # Convert the training and testing sets to lists\n",
    "    input_train = train['parameter_files'].to_list()\n",
    "    input_test = test['parameter_files'].to_list()\n",
    "    output_train = train['h5_files'].to_list()\n",
    "    output_test = test['h5_files'].to_list()\n",
    "\n",
    "    return input_train, input_test, output_train, output_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtaining logs of stellar mass, and ratio of dust to stellar mass\n",
    "parameter_files = [file for file in os.listdir(\"../../../data/radiative_transfer/input/\") if file.startswith(\"parameters\")][:100]\n",
    "h5_files = [file for file in os.listdir(\"../../../data/radiative_transfer/output/\") if file.startswith(\"data\")][:100]\n",
    "\n",
    "# splitting the dataset into training and testing sets by filename - this is to ensure every inclination of a run is in the same set\n",
    "# parameter_train, parameter_test, h5_train, h5_test = train_test_split(parameter_files, h5_files, test_size=0.2, random_state=0)\n",
    "parameter_train, parameter_test, h5_train, h5_test = split_dataset(parameter_files, h5_files)\n",
    "\n",
    "wavelength, h5_train_data = generate_dataset(pd.DataFrame(columns=[\"log_mstar\", \"log_mdust_over_mstar\", \"theta\", \"n\", \"flux\", \"r\"]), parameter_train, h5_train)\n",
    "wavelength, h5_test_data = generate_dataset(pd.DataFrame(columns=[\"log_mstar\", \"log_mdust_over_mstar\", \"theta\", \"n\", \"flux\", \"r\"]), parameter_test, h5_test)\n",
    "\n",
    "#split training and testing sets into inputs and outputs\n",
    "train_inputs, train_outputs = h5_train_data[[\"log_mstar\", \"log_mdust_over_mstar\", \"theta\"]], h5_train_data[[\"n\", \"flux\", \"r\"]]\n",
    "test_inputs, test_outputs = h5_test_data[[\"log_mstar\", \"log_mdust_over_mstar\", \"theta\"]], h5_test_data[[\"n\", \"flux\", \"r\"]]\n",
    "\n",
    "#convert training and testing data into numpy arrays\n",
    "train_inputs, test_inputs = train_inputs.to_numpy(), test_inputs.to_numpy()\n",
    "train_inputs, test_inputs = torch.Tensor(train_inputs).to(device), torch.Tensor(test_inputs).to(device)\n",
    "\n",
    "train_outputs, test_outputs = convert_to_tensor(train_outputs).to(device), convert_to_tensor(test_outputs).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise, train, and test the model\n",
    "model_attributes = initialise_model(1000, 0.3, lr=0.01)\n",
    "model = train_model(model_attributes, train_inputs, train_outputs, 25, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_n_results, std_n_results, mean_f_results, std_f_results, mean_r_results, std_r_results = test_model(model, model_attributes, input_test=test_inputs, output_test=test_outputs)\n",
    "test_outputs = test_outputs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the predicted sersic index, flux, and half-radius against wavelength on separate axes, but same figure: for all of the first galaxy's inclinations\n",
    "for i in range(10, len(mean_n_results), 10):\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(10, 10))\n",
    "    axs[0].plot(wavelength, mean_n_results[:,i], label=f\"Predicted Mean Model\")\n",
    "    axs[0].plot(wavelength, test_outputs[i, 0], label=f\"Actual Data, $\\\\log(M_*/M_\\\\odot)$ = {test_inputs[i,0]:.2f}, $\\\\log(M_d/M_*)$ = {test_inputs[i,1]:.2f}, $\\\\theta$ = {test_inputs[i,2]:.2f}\")\n",
    "    axs[0].set_xlabel(\"Wavelength / $\\\\mu m$\")\n",
    "    axs[0].set_ylabel(\"Normalised Sersic Index\")\n",
    "    axs[0].set_xscale(\"log\")\n",
    "    axs[0].set_ylim(-2, 2)\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(wavelength, mean_f_results[:,i], label=f\"Predicted Mean Model\")\n",
    "    axs[1].plot(wavelength, test_outputs[i, 1], label=f\"Actual Data, $\\\\log(M_*/M_\\\\odot)$ = {test_inputs[i,0]:.2f}, $\\\\log(M_d/M_*)$ = {test_inputs[i,1]:.2f}, $\\\\theta$ = {test_inputs[i,2]:.2f}\")\n",
    "    axs[1].set_xlabel(\"Wavelength / $\\\\mu m$\")\n",
    "    axs[1].set_ylabel(\"Normalised Flux / W/$m^2$\")\n",
    "    axs[1].set_xscale(\"log\")\n",
    "    axs[1].set_ylim(-1, 3)\n",
    "    axs[1].legend()\n",
    "\n",
    "    axs[2].plot(wavelength, mean_r_results[:,i], label=f\"Predicted Mean Model\")\n",
    "    axs[2].plot(wavelength, test_outputs[i, 2], label=f\"Actual Data, $\\\\log(M_*/M_\\\\odot)$ = {test_inputs[i,0]:.2f}, $\\\\log(M_d/M_*)$ = {test_inputs[i,1]:.2f}, $\\\\theta$ = {test_inputs[i,2]:.2f}\")\n",
    "    axs[2].set_xlabel(\"Wavelength / $\\\\mu m$\")\n",
    "    axs[2].set_ylabel(\"Normalised Half-light radius / kpc\")\n",
    "    axs[2].set_xscale(\"log\")\n",
    "    axs[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
